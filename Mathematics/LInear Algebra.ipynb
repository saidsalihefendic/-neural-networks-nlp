{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra\n",
    "\n",
    "In this notebook, we will have an overview of Linear Algebra as it is being one of the fundamental mathematics behind machine learning and deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We want to find a way to construct a set of objects and a set of rules to manipulate these objects. The field of mathematics that studies this is called _algebra_. **Linear algebra** is the study of vectors and certain rules to manipulate vectors. The vectors we are familiar with are *geometric vectors*, e.g. $\\vec{x}$ and $\\vec{y}$ from *analytic geometry*. In linear algebra, we will discuss about more general concepts of vectors $\\textbf{x}$ and $\\textbf{y}$.\n",
    "\n",
    "In general, vectors are special objects that can be added together and multiplied by scalars to produce another object of the same kind. From a mathematical viewpoint, any object that satisfies these two properties can be considered a vector. Some examples of vectors are _geometric vectors, polynomials, audio signals_ and *elements of $\\mathbb{R}^n$*, which is the concept we focus on in this notebook. For instance\n",
    "\n",
    "$$\\textbf{a} = \\left [ \n",
    "\\begin{matrix}\n",
    "9 \\\\\n",
    "3 \\\\\n",
    "4\n",
    "\\end{matrix}\\right ] \\in \\mathbb{R}^3$$\n",
    "\n",
    "is an example of a triplet of numbers. Adding two vectors $\\textbf{a}, \\textbf{b} \\in \\mathbb{R}^n$ component-wise results in another vector: $\\textbf{a} + \\textbf{b} = \\textbf{c} \\in \\mathbb{R}^n$. Moreover, multiplying $\\textbf{a} \\in \\mathbb{R}^n$ by $\\lambda \\in \\mathbb{R}$ results a scaled vector $\\lambda\\textbf{a} \\in \\mathbb{R}^n$.\n",
    "\n",
    "Most of the concepts behind linear algebra and major benefit is that it kind of represents the arrays in the computer systems. Throughout this notebook, we will observe the tools derived from Linear Algebra that will later be put to use on Machine Learning and Deep Neural Networks, which is the point of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems of Linear Equations\n",
    "\n",
    "Systems of Linear Equations play a central part of linear algebra. Many problems can be formulated as systems of linear equations and linear algebra gives us the tools for solving them. The main point is to find a way to discuss the solution set of the systems in order for us to proceed with solving the problem.\n",
    "\n",
    "Let us look what a system of linear equations looks like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "a_{11}x_1 + a_{12}x_2 + \\dots a_{1n}x_n = b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\dots a_{2n}x_n = b_2 \\\\\n",
    "\\vdots \\\\\n",
    "a_{m1}x_1 + a_{m2}x_2 + \\dots a_{mn}x_n = b_m \\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "This is the general form of a _system of linear equations_. $x_1, x_2, \\dots, x_n$ are the _unknowns_ of this system and $a_{ij}, i = \\overline{1, m}, j = \\overline{1, n}$ are called _coefficients_. Every $n$-tuple $(x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$ that satisfies every linear equation in the system is a _solution_ of the linear equations system. \n",
    "\n",
    "With closer inspection, a system of linear equations can have _no solution, unique solution_ and _infinitely many solutions_ (TODO: Give examples)\n",
    "\n",
    "For a system of linear equations that doesn't have a solution, we can apply Linear Regression algorithm that apporximately solves the linear equations system. \n",
    "\n",
    "For a systemic approach to solving systems of linear equations, we will introduce a useful compact notation. We collect the coefficients $a_{ij}$ into vectors and collect the vectors into _matrices_. We write the system as such\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "a_{11} \\\\\n",
    "a_{12} \\\\\n",
    "\\vdots \\\\\n",
    "a_{1n}\n",
    "\\end{bmatrix}x_1 +\n",
    "\\begin{bmatrix}\n",
    "a_{21} \\\\\n",
    "a_{22} \\\\\n",
    "\\vdots \\\\\n",
    "a_{2n}\n",
    "\\end{bmatrix}x_2 + \\dots + \n",
    "\\begin{bmatrix}\n",
    "a_{m1} \\\\\n",
    "a_{m2} \\\\\n",
    "\\vdots \\\\\n",
    "a_{mn}\n",
    "\\end{bmatrix}x_n = \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_m\n",
    "\\end{bmatrix}\n",
    "\\iff\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Since we have introduced the compact notation of system of linear equations, we will look now closer on the _matrices_ and define computation rules. Note that matrices are extremely important for further study of Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "Matrices play a central role in linear algebra. They can be used to compactly represent systems of linear equations, but they also represent linear functions or more known as _linear mappings_.\n",
    "\n",
    "**Definition 1** With $m, n \\in \\mathbb{N}$, a real-valued $(m, n)$ _matrix_ $\\textbf{A}$ is an $m\\cdot n$-tuple of elements $a_{ij}, i = \\overline{1, m}, j = \\overline{1, n}$ which is ordered according to a rectangular scheme consisting of $m$ rows and $n$ columns:\n",
    "\n",
    "$$\\textbf{A} = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn} \\\\\n",
    "\\end{bmatrix}, a_{ij} \\in \\mathbb{R}, i = \\overline{1, m}, j = \\overline{1, n}$$\n",
    "\n",
    "By convention, $(1, n)$-matrices are called _rows_ and $(m, 1)$-matrices are called _columns_. These special matrices are also called *row vector* and *column vector*, respectively.\n",
    "\n",
    "$\\mathbb{R}^{m\\times n}$ is the set of all real-valued $(m, n)$-matrices.\n",
    "\n",
    "The sum of two matrices $\\textbf{A}, \\textbf{B} \\in \\mathbb{R}^{m \\times n}$ is defined as the element-wise sum, i.e.,\n",
    "\n",
    "$$\\textbf{A} + \\textbf{B} := \n",
    "\\begin{bmatrix}\n",
    "a_{11} + b_{11} & a_{12} + b_{12} & \\dots & a_{1n} + b_{1n} \\\\\n",
    "a_{21} + b_{21} & a_{22} + b_{22} & \\dots & a_{2n} + b_{2n} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "a_{m1} + b_{m1} & a_{m2} + b_{m2} & \\dots & a_{mn} + b_{mn} \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "For matrices $\\textbf{A} \\in \\mathbb{R}^{m \\times n}, \\textbf{B} \\in \\mathbb{R}^{n \\times k}$, the elements $c_{ij}$ of the product $\\textbf{C} = \\textbf{A}\\textbf{B} \\in \\mathbb{R}^{m \\times k}$ are computed as\n",
    "\n",
    "$$c_{ij} = \\sum_{l = 1}^{n}a_{il}b_{lj}, i = \\overline{1, m}, j = \\overline{1, k}$$\n",
    "\n",
    "This is also known as the _dot product_.\n",
    "\n",
    "Matrices can only be multiplied if their \"neighbouring\" dimensions match. For instance, a $n \\times k$-matrix $\\textbf{A}$ can be multiplied with $k \\times m$-matrix $\\textbf{B}$, but only from the left side\n",
    "\n",
    "$$\\textbf{A} \\in \\mathbb{R}^{n \\times k}, \\textbf{B} \\in \\mathbb{R}^{k \\times m} \\rightarrow \\textbf{AB} = \\textbf{C}, \\textbf{C} \\in \\mathbb{R}^{n \\times m}$$.\n",
    "\n",
    "The product $\\textbf{B}\\textbf{A}$ is not defined if $m \\neq n$ since the neighbouring dimensions don't match.\n",
    "\n",
    "From this, the product is also not commutative $\\textbf{A}\\textbf{B} \\neq \\textbf{B}\\textbf{A}$.\n",
    "\n",
    "**Definition 2.** In $\\mathbb{R}^{n\\times n}$ we define the _identity matrix_\n",
    "\n",
    "$$\\textbf{I}_n := \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 0 & \\dots & 1 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "as the $n \\times n$-matrix containing 1 on the diagonal and 0 everywhere else.\n",
    "\n",
    "Now that we have defined matrix multiplication, matrix addition and the identity matrix, let us look at some properties of matrices:\n",
    "\n",
    "- Associativity\n",
    "    - $\\forall \\textbf{A} \\in \\mathbb{R}^{m \\times n}, \\textbf{B} \\in \\mathbb{R}^{n \\times p}, \\textbf{C} \\in \\mathbb{R}^{p \\times q} : (\\textbf{A}\\textbf{B})\\textbf{C} = \\textbf{A}(\\textbf{B}\\textbf{C})$\n",
    "- Distributivity\n",
    "    - $\\forall \\textbf{A}, \\textbf{B} \\in \\mathbb{R}^{m \\times n}, \\textbf{C}, \\textbf{D} \\in \\mathbb{R}^{n \\times p} : \n",
    "    \\begin{matrix}\n",
    "    (\\textbf{A} + \\textbf{B})\\textbf{C} = \\textbf{A}\\textbf{C} + \\textbf{B}\\textbf{C} \\\\\n",
    "    \\textbf{A}(\\textbf{C} + \\textbf{D}) = \\textbf{A}\\textbf{C} + \\textbf{A}\\textbf{D} \\end{matrix}$\n",
    "    \n",
    "- Multiplication with the identity matrix:\n",
    "    - $\\forall \\textbf{A} \\in \\mathbb{R}^{m \\times n}: \\textbf{I}_m\\textbf{A} = \\textbf{A}\\textbf{I}_n = \\textbf{A}$\n",
    "    \n",
    "Note that $\\textbf{I}_m \\neq \\textbf{I}_n$ for $m \\neq n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3** Consider a square matrix $\\textbf{A} \\in \\mathbb{R}^{n \\times n}$.\n",
    " Let matrix $\\textbf{B} \\in \\mathbb{R}^{n \\times n}$ have the property that $\\textbf{A}\\textbf{B} = \\textbf{I}_n = \\textbf{B}\\textbf{A}$. $\\textbf{B}$ is called _inverse_ of $\\textbf{A}$ and denoted by $\\textbf{A}^{-1}$.\n",
    " \n",
    "Not every matrix $\\textbf{A}$ possesses an inverse $\\textbf{A}^{-1}$. If this inverse does exist, $\\textbf{A}$ is called *regular/invertible/nonsingular*, otherwise *singular/noninvertible*. When the matrix inverse exists, it is unique, which can be shown as the consequence of the definition. Consider for a regular matrix $\\textbf{A}$ that we have $\\textbf{A}_1$ and $\\textbf{A}_2$ that are the inverses of $\\textbf{A}$. Since $\\textbf{A}_1$ is the inverse, then it follows $\\textbf{A}\\textbf{A}_1 = \\textbf{I}_n = \\textbf{A}_1\\textbf{A}$. For $\\textbf{A}_2$ it follows analogously to $\\textbf{A}_1$. We have\n",
    "\n",
    "$$\\textbf{A}\\textbf{A}_1 = \\textbf{I}_n = \\textbf{A}\\textbf{A}_2 \\Rightarrow \\textbf{A}\\textbf{A}_1 = \\textbf{A}\\textbf{A}_2$$\n",
    "\n",
    "Multiplying both sides of equation on the left by either $\\textbf{A}_1$ or $\\textbf{A}_2$, it doesn't matter. We will choose $\\textbf{A}_1$, we get\n",
    "\n",
    "$$\\textbf{A}_1\\textbf{A}\\textbf{A}_1 = \\textbf{A}_1\\textbf{A}\\textbf{A}_2 \\Rightarrow \\textbf{I}_n\\textbf{A}_1 = \\textbf{I}_n\\textbf{A}_2 \\Rightarrow \\textbf{A}_1 = \\textbf{A}_2$$.\n",
    "\n",
    "From the definition of inverse matrices and identity matrix, we get that there can't be two inverse matrices for regular matrix $\\textbf{A} \\in \\mathbb{R}^{n \\times n}$, so the inverse of matrix $\\textbf{A}$ is unique.\n",
    "\n",
    "We will have a look at when the matrix $\\textbf{A}$ is regular by looking at its determinant in the future notebooks.\n",
    "\n",
    "**Definition 4** For $\\textbf{A} \\in \\mathbb{R}^{m \\times n}$ the matrix $\\textbf{B} \\in \\mathbb{R}^{n \\times m}$ with $b_{ij} = a_{ji}$ is called the *transpose* of $\\textbf{A}$. We write $\\textbf{B} = \\textbf{A}^T$.\n",
    "\n",
    "The following are the important properties of inverses and transposes:\n",
    "\n",
    "$$\\textbf{A}\\textbf{A}^{-1} = I = \\textbf{A}^{-1}\\textbf{A} \\\\\n",
    "(\\textbf{A}\\textbf{B})^{-1} = \\textbf{B}^{-1}\\textbf{A}^{-1} \\\\\n",
    "(A + B)^{-1} \\neq A^{-1} + B^{-1} \\\\\n",
    "(\\textbf{A}^T)^T = \\textbf{A} \\\\\n",
    "(\\textbf{A} + \\textbf{B})^T = \\textbf{A}^T + \\textbf{B}^T \\\\\n",
    "(AB)^T = \\textbf{B}^T\\textbf{A}^T \\\\\n",
    "(\\textbf{A}^{-1})^T = (\\textbf{A}^T)^{-1} = \\textbf{A}^{-T}$$\n",
    "\n",
    "\n",
    "**Definition 5** A matrix $\\textbf{A} \\in \\mathbb{R}^{n \\times n}$ is *symmetric* if $\\textbf{A} = \\textbf{A}^T$.\n",
    "\n",
    "The sum of symmetric matrices  gives us symmetric matrix, however, the product of symmetric matrices is not always symmteric.\n",
    "\n",
    "Let $\\textbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\lambda \\in \\mathbb{R}$. Then $\\lambda\\textbf{A} = K, K_{ij} = \\lambda a_{ij}$. Practically, this means we scale the element of matrix $\\textbf{A}$ by $\\lambda$.\n",
    "\n",
    "For $\\lambda, \\psi \\in \\mathbb{R}$, the following holds:\n",
    "\n",
    "- Associativity\n",
    "    - $(\\lambda\\psi)\\textbf{C} = \\lambda(\\psi\\textbf{C}), \\textbf{C} \\in \\mathbb{R}^{m \\times n}$\n",
    "    - $\\lambda(\\textbf{B}\\textbf{C}) = (\\lambda\\textbf{B})\\textbf{C} = \\textbf{B}(\\lambda\\textbf{C}) = (\\textbf{B}\\textbf{C})\\lambda, \\textbf{B} \\in \\mathbb{R}^{m \\times n}, \\textbf{C} \\in \\mathbb{R}^{n \\times k}$\n",
    "    - $(\\lambda\\textbf{C})^T = \\textbf{C}^T\\lambda^T = \\textbf{C}^T\\lambda = \\lambda\\textbf{C}^T$ since $\\lambda = \\lambda^T, \\forall \\lambda \\in \\mathbb{R}$\n",
    "- Distributivity\n",
    "    - $(\\lambda + \\psi)\\textbf{C} = \\lambda\\textbf{C} + \\psi\\textbf{C}, \\textbf{C} \\in \\mathbb{R}^{m \\times n}$\n",
    "    - $\\lambda(\\textbf{B} + \\textbf{C}) = \\lambda\\textbf{B} + \\lambda\\textbf{C}, \\textbf{B}, \\textbf{C} \\in \\mathbb{R}^{m \\times n}$\n",
    "    \n",
    "Now that we have the basics of matrices and its properties defined, from the above we get the general matrix form of system of linear equations as $\\textbf{A}\\textbf{x} = \\textbf{b}$, where the product $\\textbf{A}\\textbf{x}$ is a linear combination of the columns of $\\textbf{A}$.\n",
    "\n",
    "The matrices are extremely useful in various areas of Computer Science and especially in Machine Learning, where we will have to represent layers of neural networks and find the forward propagation of the neural network, which uses matrix multiplication we have discussed above for representing edges and it's weight with the vector being the neurons in the layers. We will revisit the matrices again when we start speaking about neural networks and in some machine learning algorithms such as the Normal equation of the Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Systems of Linear equations\n",
    "\n",
    "Now that we have the general form of the system of linear equations and its compact notation, let's see how we can actually solve them.\n",
    "\n",
    "Most systems of linear equations can have infinitely many or no solution, depending on how the linear equations behave against other linear equations in the system. we can generally assume that if there are more unknowns than there are linear equations, we would expect infinitely many solutions and if the system of linear equations has more linear equations than there are uknown, there is a good chance the system doesn't have a solution. But note that none of this guarantees, we just assume that there is.\n",
    "\n",
    "The general approach for solving a system of linear equations is:\n",
    "\n",
    "1. Find a particular solution to $\\textbf{A}\\textbf{x} = \\textbf{b}$\n",
    "2. Find all non-trivial solutions to $\\textbf{A}\\textbf{x} = 0$\n",
    "3. Combine the solutions from steps 1. and 2. to the general solution\n",
    "\n",
    "We need to note that neither the particular nor general solution are unique.\n",
    "\n",
    "A key to solving a system of linear equations are *elementary transformations* that keep the solution set the same, but that transform the equation system into a simpler form:\n",
    "\n",
    "- Exchange of two equations (rows in the matrix representing the system of equations)\n",
    "- Multiplication of an equation (row) with a constant $\\lambda \\in \\mathbb{R} \\setminus \\{0\\}$\n",
    "- Addition of two equations (rows)\n",
    "\n",
    "The leading coefficient of a row (first nonzero number from the left) is called the *pivot* and is always strictly to the right of the _pivot_ of the row above it. Therefore, any equation system in row-echelon form always has a \"staircase\" structure.\n",
    "\n",
    "**Definition 6** A matrix is in *row-echelon* form if\n",
    "- All rows that contain only zeros are at the bottom of the matrix; correspondingly, all rows that contain at least one nonzero element are on top of rows that contain zeros\n",
    "- Looking at nonzero rows only, the first nonzero number from the left is always strictly to the right of the pivot of the row above it\n",
    "\n",
    "The variables corresponding to the pivots in the row-echelon form are called *basic variables* and the other variables are *free variables*.\n",
    "\n",
    "The row-echelon form is useful when we want to determine a particular solution. To do this, we express the right-hand side of the equation system using the pivot columns such that $\\textbf{b} = \\sum_{i = 1}^{P}\\lambda_i\\textbf{p}_i$, where $\\textbf{p}_i, i = \\overline{1, P}$ are the pivot columns. The $\\lambda_i$ are determined easiest if we start with the rightmost pivot columns and work our way to the left.\n",
    "\n",
    "An equation system is in *reduced row-echelon form* if:\n",
    "- It is in row-echelon form\n",
    "- Every pivot is 1\n",
    "- The pivot is the only non-zero entry in its column\n",
    "\n",
    "The reduced row-echelon form will play an important role later because it allows us to determine the general solution of a system of linear equations in a straightforward way.\n",
    "\n",
    "*Gaussian elimination* is an algorithm that performs elementary transformations to bring a system of linear equations into reduced row-echelon form.\n",
    "\n",
    "#### Calculating inverse\n",
    "\n",
    "To compute the inverse $\\textbf{A}^{-1}$ of $\\textbf{A} \\in \\mathbb{R}^{n \\times n}$, we need to find matrix $\\textbf{X}$ that satisfies $\\textbf{A}\\textbf{X} = \\textbf{I}_n$. Then, $\\textbf{X} = \\textbf{A}^{-1}$.\n",
    " We can write this down as a set of simultaneous linear equations $\\textbf{A}\\textbf{X} = \\textbf{I}_n$, where we solve $\\textbf{X} = [x_1|\\cdots|x_n]$ We use the augmented matrix notation for a compact representation of this set of systems of linear equations and obtain\n",
    " \n",
    "$$[\\textbf{A}|\\textbf{I}_n] \\sim \\cdots \\sim [\\textbf{I}_n | \\textbf{A}^{-1}]$$\n",
    "\n",
    "This means that if we bring the augmented equation system into reduced row-echelon form, we can read out the inverse on the right-hand side of the equation system. Hence, determining the inverse of matrix is equivalent to solving systems of linear equations.\n",
    "\n",
    "#### Algorithms for Solving a System of Linear Equations\n",
    "\n",
    "We make an assumption that the solution for a system of linear equations of the form $\\textbf{A}\\textbf{x} = \\textbf{b}$. If there happens to be no solution, we can use Linear regression to find an approximate solutions. which will be discussed in the future notebooks.\n",
    "\n",
    "In special cases, we may be able to determine the inverse $\\textbf{A}^{-1}$, such that the solution $\\textbf{A}\\textbf{x} = \\textbf{b}$ is given as $\\textbf{x} = \\textbf{A}^{-1}\\textbf{b}$. However, this is only possible if $\\textbf{A}$ is a regular matrix, which is often not the case. Otherwise, under mild assumptions that $\\textbf{A}$ needs to have linearly independent columns, we can use transformation:\n",
    "\n",
    "$$\\textbf{A}\\textbf{x} = \\textbf{b} \\equiv \\textbf{A}^T\\textbf{A}\\textbf{x} = \\textbf{A}^T\\textbf{b} \\equiv \\textbf{x} = (\\textbf{A}^T\\textbf{A})^{-1}\\textbf{A}^T\\textbf{b}$$.\n",
    "\n",
    "and use *Moore-Penrose pseudo-inverse* $(\\textbf{A}^T\\textbf{A})^{-1}\\textbf{A}^T$ to determine the solution that solve $\\textbf{A}\\textbf{x} = \\textbf{b}$, which also corresponds to the minimum norm least-squares solution. But the major disadvantage of this approach is that it requires many computations for matrix-matrix product and computing the inverse $\\textbf{A}^T\\textbf{A}$. Moreover, for reasons of numerical precision it is generally not recommended to compute the inverse or pseudo-inverse. we discuss now the alternatives.\n",
    "\n",
    "Gaussian elimination plays an important role when computing determinants, checking whether a set of vectors is linearly independent, computing the inverse of matrix, computing the rank of matrix and determining a basis of a vector space. Gaussian elimination is an intuitive and constructive way to solve a system of linear equations with thousands of variables. However, for systems with millions of variables, it is impractical as the required number of arithmetic operations scales cubically in the number of simultaneous equations.\n",
    "\n",
    "In practice, systems of many linear equations are solved indirectly, by either stationary iterative methods such as Richardson, Jacobi and Gauss-Siedel and the successive over-relaxation method, or Krylov subspace methods, such as conjugate gradients, generalized minimal residual, or biconjugate gradients. \n",
    "\n",
    "Let $\\textbf{x}_*$ be a solution of $\\textbf{A}\\textbf{x} = \\textbf{b}$. The key idea of these iterative methods is to set up an iteration of the form\n",
    "\n",
    "$$\\textbf{x}^{(k + 1)} = \\textbf{C}\\textbf{x}^k + d$$\n",
    "\n",
    "for suitabe $\\textbf{C}$ and $d$ that reduces the residual error $|| \\textbf{x}^{(k + 1)} - \\textbf{x}_*||$ in every iteration and converges to $\\textbf{x}_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong et al., 2020. Mathematics for Machine Learning. Cambridge University Press\n",
    "\n",
    "Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016. Deep Learning, MIT Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
